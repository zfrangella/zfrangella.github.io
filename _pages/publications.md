---
title: "Publications"
permalink: "/publications/"
layout: single
---
Here is a list of my publications (organized by venue) and preprints. <br>
<br>
\* denotes equal contribution

## Thesis
- [Randomized Numerical Linear Algebra for Large-Scale Optimization](https://github.com/zfrangella/zfrangella.github.io/files/thesis.pdf)<br>
  Zachary Frangella <br>
 _Stanford University Thesis, 2025_

## Journal Publications
- [PROMISE: Preconditioned Stochastic Optimization Methods by Incorporating Scalable Curvature Estimates](https://jmlr.org/papers/v25/23-1187.html)<br>
  Zachary Frangella\*, Pratik Rathore\*, Shipu Zhao, Madeleine Udell<br>
  _JMLR, 2024_

- [SketchySGD: Reliable Stochastic Optimization via Randomized Curvature Estimates](https://epubs.siam.org/doi/10.1137/23M1575330)<br>
  Zachary Frangella, Pratik Rathore, Shipu Zhao, Madeleine Udell<br>
  _SIMODS, 2024_

- [Randomized Nyström Preconditioning](https://epubs.siam.org/doi/10.1137/21M1466244)<br>
  Zachary Frangella, Joel Tropp, Madeleine Udell<br>
  _SIMAX, 2023_

## Conference Publications
- [CRONOS: Enhancing Deep Learning with Scalable GPU Accelerated Convex Neural Networks](https://openreview.net/pdf?id=YfLzYczAo3)<br>
  Miria Feng, Zachary Frangella, Mert Pilanci<br>
  _NeurIPS, 2024_
  
- [Challenges in Training PINNs: A Loss Landscape Perspective](https://proceedings.mlr.press/v235/rathore24a.html)<br>
  Pratik Rathore, Weimu Lei, Zachary Frangella, Lu Lu, Madeleine Udell<br>
  _ICML, 2024, Oral (top 1.5% of submissions)_

- [NysADMM: faster composite convex optimization via low-rank approximation](https://proceedings.mlr.press/v162/zhao22a.html)<br>
  Shipu Zhao\*, Zachary Frangella\*, Madeleine Udell<br>  
  _ICML, 2022_

- [Can we globally optimize cross-validation loss? Quasiconvexity in ridge regression](https://proceedings.neurips.cc/paper/2021/file/cc298d5bc587e1b650f80e10449ee9d5-Paper.pdf)<br>  
  William T. Stephenson, Zachary Frangella, Madeleine Udell, Tamara Broderick<br>  
  _NeurIPS, 2021_

## In the Pipeline
- [On the Linear Convergence of Generalized Newton Inexact ADMM](https://arxiv.org/pdf/2302.03863)<br>
  Zachary Frangella, Theo Diamandis, Bartolomeo Stellato, Madeleine Udell<br>
  _Submitted_

- [GeNIOS: an (almost) second-order operator-splitting solver for large-scale convex optimization](https://arxiv.org/pdf/2310.08333)<br>  
  Theo Diamandis, Zachary Frangella, Shipu Zhao, Bartolomeo Stellato, Madeleine Udell<br>  
  _Submitted_

- [Robust, randomized preconditioning for kernel ridge regression](https://arxiv.org/pdf/2304.12465)<br>
  Mateo Diaz, Ethan N. Epperly, Zachary Frangella, Joel A. Tropp, Robert J. Webber<br>  
  _Submitted_

- [Enhancing Physics-Informed Neural Networks Through Feature Engineering](https://arxiv.org/pdf/2502.07209)<br>
  Shaghayegh Fazliani, Zachary Frangella, Madeleine Udell<br>
  _Submitted_

- [Have ASkotch: A Neat Solution for Large-scale Kernel Ridge Regression](https://arxiv.org/pdf/2407.10070?)<br>  
  Pratik Rathore, Zachary Frangella, Jiaming Yang, Michał Dereziński, Madeleine Udell<br>
  _Submitted_

- [Turbocharging Gaussian Process Inference with Approximate Sketch-and-Project](https://arxiv.org/pdf/2505.13723?)<br>
 Pratik Rathore, Zachary Frangella, Sachin Garg, Shaghayegh Fazliani, Michał Dereziński, Madeleine Udell<br>
 _Submitted_

- [SAPPHIRE: Preconditioned Stochcastic Variance Reduction for Faster Large-Scale Statistical Learning]()<br>
 Jingruo Sun, Zachary Frangella, Madeleine Udell<br>
 _Submitted_
